{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3614442-a3da-403f-86c4-b3679c3c4e07",
   "metadata": {},
   "source": [
    "# Extracting PD-like templates from Wikimedia Commons Files\n",
    "This notebook retrieves all files in the category `Media from Delpher` on Wikimedia Commons, \n",
    "then checks the wikitext of each file for public domain (PD) templates (templates that tyoically start with `{{PD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting public domain (PD)-like templates from Wikimedia Commons files\n",
    "========================================================================\n",
    "\n",
    "Purpose:\n",
    "--------\n",
    "This script identifies potentially public domain (PD) or PD-like license templates\n",
    "in Wikimedia Commons files categorized under:\n",
    "\n",
    "    - Category:Media from Delpher\n",
    "    - Excluding: Category:Scans from the Internet Archive\n",
    "\n",
    "These templates are often indicative of public domain status, but not explicitly\n",
    "tagged as such. The script extracts them, alongside simplified creation/publication\n",
    "dates, for review and documentation purposes.\n",
    "\n",
    "Key Features:\n",
    "-------------\n",
    "- Uses the MediaWiki API to search for Commons files in the desired category.\n",
    "- Fetches the raw wikitext of each file page.\n",
    "- Isolates wrapper templates like {{Information}}, {{Photograph}}, {{Artwork}}, and {{Book}}.\n",
    "- Extracts relevant templates from top-level usage or embedded fields like:\n",
    "  - |permission=\n",
    "  - |date=\n",
    "  - |publication date=\n",
    "- Handles multiline and nested template values reliably.\n",
    "- Extracts a simplified creation date from various formats:\n",
    "  - {{circa|1930}}, {{taken on|1918-12-21}}, {{other date|between|1890|1900}}, etc.\n",
    "- Supports date formats: YYYY, YYYY-MM, YYYY-MM-DD\n",
    "- Returns the most recent valid year if multiple are present.\n",
    "- Excludes known irrelevant templates via a robust filtering system.\n",
    "- Outputs results to:\n",
    "  - Console (one line per file with all extracted info)\n",
    "  - Excel file (`*_commons_templates_output_<date>.xlsx`) with URLs and linked templates\n",
    "\n",
    "Output:\n",
    "-------\n",
    "- File URL\n",
    "- Number of detected templates\n",
    "- Simplified creation or publication date\n",
    "- Template names and links to their Commons documentation pages\n",
    "\n",
    "Dependencies:\n",
    "-------------\n",
    "- Python 3.7+\n",
    "- `requests`, `re`, `pandas`, `openpyxl`\n",
    "\n",
    "Author:\n",
    "-------\n",
    "- Olaf Janssen, Wikimedia coordinator @KB national library of the Netherlands (via ChatGPT)\n",
    "- Last updated: 9 April 2025\n",
    "- User-Agent: OlafJanssenBot/1.0\n",
    "\n",
    "License:\n",
    "--------\n",
    "This script is released into the public domain (CC0-style). Free to reuse, adapt, and distribute.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import openpyxl\n",
    "\n",
    "# List of templates to exclude (case-insensitive, supports wildcards for regex filtering)\n",
    "EXCLUDED_TEMPLATES = {\n",
    "    '1937',\n",
    "    '1937 03 17',\n",
    "    'after',\n",
    "    'anonymous',\n",
    "    'author',\n",
    "    'before',\n",
    "    'between',\n",
    "    'bijbelsche kunst',\n",
    "    'bildindex',\n",
    "    'boijmansonline',\n",
    "    'booknavibar',\n",
    "    'border is intentional',\n",
    "    'chefs d\\'oeuvre de la collection d.g. van beuningen',\n",
    "    'cite news',\n",
    "    'circa',\n",
    "    'collective work',\n",
    "    'complex date',\n",
    "    'creator',\n",
    "    'crop for wikidata',\n",
    "    'date',\n",
    "    'dead link',\n",
    "    'de collectie verrijkt',\n",
    "    'de jérôme bosch à rembrandt, peintures et dessins du musée boymans de rotterdam',\n",
    "    'de minimis',\n",
    "    'deminimis',\n",
    "    'delpher',\n",
    "    'djvu',\n",
    "    'daumier register',\n",
    "    'dutch art 1450–1900',\n",
    "    'extracted',\n",
    "    'extracted from',\n",
    "    'fop-pakistan',\n",
    "    'fourcaud (1)',\n",
    "    'fraenger',\n",
    "    'friedländer',\n",
    "    'haverman, hendrik johannes',\n",
    "    'het wonder, miracula christi',\n",
    "    'hieronymus bosch, the complete paintings and drawings',\n",
    "    'hieronymus bosch, visions of genius',\n",
    "    'honderd jaar museum boymans, rotterdam, meesterwerken uit de verzameling d.g. van beuningen',\n",
    "    'i18n/as',\n",
    "    'image extracted',\n",
    "    'imagenote',\n",
    "    'imagenoteend',\n",
    "    'insignia',\n",
    "    'jeroen bosch, noord-nederlandsche primitieven',\n",
    "    'jérôme bosch (fierens-vevaert)',\n",
    "    'jheronimus bosch (1967)',\n",
    "    'jheronimus bosch (2001)',\n",
    "    'jheronimus bosch alle schilderijen en tekeningen',\n",
    "    'kersttentoonstelling (1927-1928)',\n",
    "    'kik-irpa',\n",
    "    'kunstschatten uit nederlandse verzamelingen',\n",
    "    'la collection goudstikker (june 1927)',\n",
    "    'langswitch',\n",
    "    'les primitifs flamands',\n",
    "    'location',\n",
    "    'marijnissen',\n",
    "    'object location',\n",
    "    'onze afgevaardigden 1909',\n",
    "    'onze afgevaardigden 1913',\n",
    "    'onze musici',\n",
    "    'onze musici (1923)',\n",
    "    'original',\n",
    "    'original caption',\n",
    "    'original description',\n",
    "    'original description page',\n",
    "    'other date',\n",
    "    'otherdate',\n",
    "    'otherversion',\n",
    "    'other version',\n",
    "    'p-page',\n",
    "    'pd-algorithm',\n",
    "    'provenanceevent',\n",
    "    'retouched',\n",
    "    'retouchedpicture',\n",
    "    'retouched picture',\n",
    "    'rijksmonument',\n",
    "    'rkdimages',\n",
    "    'see more images',\n",
    "    'size',\n",
    "    'superseded',\n",
    "    'taken on',\n",
    "    'technique',\n",
    "    'tentoonstelling hieronymus bosch (1930)',\n",
    "    'tentoonstelling van oude kunst door de vereeniging van handelaren in oude kunst in nederland',\n",
    "    'tolnay',\n",
    "    'transferred from',\n",
    "    'ucfirst',\n",
    "    'uncategorized',\n",
    "    'uploaded from mobile',\n",
    "    'uploaded with derivativefx',\n",
    "    'user',\n",
    "    'van eyck to bruegel, 1400-1550',\n",
    "    'verzameling f. koenigs',\n",
    "    'vlaamsche kunst',\n",
    "    'wga'\n",
    "}\n",
    "\n",
    "WRAPPER_TEMPLATES = ['Information', 'information','Photograph','photograph', 'Artwork', 'artwork', 'Art Photo','Art photo','Book','book']\n",
    "\n",
    "LANGUAGE_TEMPLATE_PATTERN = re.compile(r'^[a-z]{2,3}$', re.IGNORECASE)\n",
    "ONZE_PATTERN = re.compile(r'^onze afgevaardigden.*$', re.IGNORECASE)\n",
    "DEFAULTSORT_PATTERN = re.compile(r'^defaultsort[: ]', re.IGNORECASE)\n",
    "COLON_TEMPLATE_PATTERN = re.compile(r'^.*:.*$') #{{Creator:Hendrik Jan Bulthuis}}, {{User:Wdwdbot}}, {{ucfirst: {{Anonymous}} or {{Template:Something}}\n",
    "\n",
    "def extract_year_from_date_string(date_str):\n",
    "    \"\"\"\n",
    "    Extracts a simplified year or century from a Wikimedia Commons-style date string.\n",
    "\n",
    "    Handles:\n",
    "    - Templates like {{circa|1939}}, {{taken on|YYYY-MM-DD}}, {{other date|...}}\n",
    "    - Flexible formats: YYYY, YYYY-MM, YYYY-MM-DD\n",
    "    - Nested templates (e.g. wrapped with {{ucfirst:...}})\n",
    "    - Returns the most recent valid 4-digit year found (1000 ≤ year ≤ 2100)\n",
    "    - Ignores metadata like:\n",
    "        - accessdate=2013, archivedate=2022\n",
    "        - {{Dead link|date=...}}, {{cite news|date=...}}, etc.\n",
    "\n",
    "    Args:\n",
    "        date_str (str): Raw wikitext string from a |date= or |publication date= field\n",
    "\n",
    "    Returns:\n",
    "        str: Parsed date (e.g., '1939', '20th century', or 'Unknown')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lower = date_str.lower()\n",
    "\n",
    "        # --- Strip metadata noise that can corrupt fallback year parsing ---\n",
    "        # Remove citation templates entirely\n",
    "        date_str = re.sub(r'\\{\\{\\s*cite\\s+(news|web|book|journal)[^\\}]*\\}\\}', '', date_str, flags=re.IGNORECASE)\n",
    "        # Remove known non-creation date fields\n",
    "        date_str = re.sub(r'\\|\\s*([Aa]ccessdate|[Aa]ccess-date|[Aa]rchivedate|[Aa]rchive-date)\\s*=\\s*[^\\|\\n]+', '', date_str, flags=re.IGNORECASE)\n",
    "        date_str = re.sub(r'\\{\\{[Dd]ead link\\|date=\\w+\\s+\\d{4}.*?\\}\\}', '', date_str, flags=re.IGNORECASE)\n",
    "\n",
    "        # --- Unwrap any ucfirst: {{...}} ---\n",
    "        nested_match = re.search(r'\\{\\{[Uu][Cc]first:\\s*(\\{\\{.*?\\}\\})\\s*\\}\\}', date_str)\n",
    "        if nested_match:\n",
    "            date_str = nested_match.group(1)\n",
    "\n",
    "        # --- {{complex date|century|20|adj1=early}} → Early 20th century ---\n",
    "        if '{{complex date' in lower:\n",
    "            century_match = re.search(r'\\|\\s*century\\s*\\|\\s*(\\d{1,2})', date_str)\n",
    "            adj_match = re.search(r'\\|\\s*adj1\\s*=\\s*(\\w+)', date_str)\n",
    "            if century_match:\n",
    "                century = int(century_match.group(1))\n",
    "                adjective = adj_match.group(1).capitalize() + ' ' if adj_match else ''\n",
    "                return f\"{adjective}{century}th century\"\n",
    "\n",
    "        # --- {{circa|1939}} → 1939 ---\n",
    "        circa_match = re.search(r'\\{\\{\\s*[Cc]irca\\s*\\|\\s*(\\d{4})\\s*\\}\\}', date_str)\n",
    "        if circa_match:\n",
    "            return circa_match.group(1)\n",
    "\n",
    "        # --- {{other date|century|16}} → 16th century ---\n",
    "        century_match = re.search(r'\\{\\{\\s*[Oo]ther date\\s*\\|\\s*century\\s*\\|\\s*(\\d{1,2})', date_str)\n",
    "        if century_match:\n",
    "            return f\"{century_match.group(1)}th century\"\n",
    "\n",
    "        # --- {{other date|?|...}} → Unknown ---\n",
    "        if re.search(r'\\{\\{\\s*[Oo]ther date\\s*\\|\\s*\\?\\s*\\}\\}', date_str):\n",
    "            return \"Unknown\"\n",
    "\n",
    "        # --- {{taken on|YYYY-MM-DD}} → YYYY ---\n",
    "        taken_on_match = re.search(r'\\{\\{\\s*[Tt]aken on\\s*\\|\\s*(\\d{4})-\\d{2}-\\d{2}', date_str)\n",
    "        if taken_on_match:\n",
    "            return taken_on_match.group(1)\n",
    "\n",
    "        # --- Catch all other {{other date|...}} variations and extract latest year ---\n",
    "        if '{{other date' in lower:\n",
    "            all_years = re.findall(r'\\d{4}', date_str)\n",
    "            valid_years = [y for y in all_years if 1000 <= int(y) <= 2100]\n",
    "            if valid_years:\n",
    "                return max(valid_years)\n",
    "\n",
    "        # --- Final fallback: any 4-digit year ---\n",
    "        fallback_years = re.findall(r'\\d{4}', date_str)\n",
    "        valid_years = [y for y in fallback_years if 1000 <= int(y) <= 2100]\n",
    "        if valid_years:\n",
    "            return max(valid_years)\n",
    "\n",
    "        return date_str.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting year from date: {e}\")\n",
    "        return date_str.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_excluded_template(name):\n",
    "    \"\"\"\n",
    "    Determines whether a template name should be excluded based on known irrelevant, generic,\n",
    "    or colon-containing patterns (e.g., {{User:...}}, {{Creator:...}}, {{ucfirst:...}}).\n",
    "\n",
    "    This function checks if the given template name:\n",
    "    - Is in the predefined `EXCLUDED_TEMPLATES` set (case-insensitive)\n",
    "    - Matches known patterns like {{DEFAULTSORT}}, {{onze afgevaardigden...}}, or language codes (e.g., \"en\", \"nl\")\n",
    "    - Contains a colon (used for namespaced or nested templates)\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the template to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the template should be excluded, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        name_lc = name.lower()\n",
    "        return (\n",
    "            name_lc in EXCLUDED_TEMPLATES\n",
    "            or DEFAULTSORT_PATTERN.match(name_lc)\n",
    "            or ONZE_PATTERN.match(name_lc)\n",
    "            or LANGUAGE_TEMPLATE_PATTERN.match(name_lc)\n",
    "            or COLON_TEMPLATE_PATTERN.match(name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking if template is excluded ({name}): {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_wikitext(title):\n",
    "    \"\"\"\n",
    "    Retrieves the raw wikitext of a Wikimedia Commons page by its title.\n",
    "\n",
    "    Uses the MediaWiki API (`action=parse`) to fetch the 'wikitext' property of the specified page.\n",
    "    If the request fails or the expected response format is missing, returns an empty string.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the Wikimedia Commons page (e.g., \"File:Example.jpg\").\n",
    "\n",
    "    Returns:\n",
    "        str: The raw wikitext of the page, or an empty string if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = 'https://commons.wikimedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'parse',\n",
    "            'page': title,\n",
    "            'prop': 'wikitext',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        headers = {\"User-Agent\": \"OlafJanssenBot/1.0 (https://commons.wikimedia.org/wiki/User:OlafJanssenBot; Python script)\"}\n",
    "        response = requests.get(url, params=params, headers=headers).json()\n",
    "        return response['parse']['wikitext']['*']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching wikitext for {title}: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "def search_files_from_category_excluding_term(include_term, exclude_term, limit=500):\n",
    "    \"\"\"\n",
    "    Searches Wikimedia Commons for files in one category while excluding files from another.\n",
    "\n",
    "    Uses the MediaWiki search API to find files (namespace 6) that are in the `include_term` category\n",
    "    but not in the `exclude_term` category. Fetches results in batches using pagination.\n",
    "\n",
    "    Args:\n",
    "        include_term (str): The name of the category to include (e.g., \"Media from Delpher\").\n",
    "        exclude_term (str): The name of the category to exclude (e.g., \"Scans from the Internet Archive\").\n",
    "        limit (int): The number of results to retrieve per API call (max 500).\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of file titles (e.g., \"File:Example.jpg\"). Empty if the request fails.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Fetching files in category '{include_term}' excluding '{exclude_term}'...\")\n",
    "    try:\n",
    "        SEARCH_URL = \"https://commons.wikimedia.org/w/api.php\"\n",
    "        offset = 0\n",
    "        all_titles = []\n",
    "\n",
    "        while True:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"list\": \"search\",\n",
    "                \"srsearch\": f\"incategory:\\\"{include_term}\\\" -incategory:\\\"{exclude_term}\\\"\",\n",
    "                \"srlimit\": limit,\n",
    "                \"srnamespace\": 6,\n",
    "                \"sroffset\": offset\n",
    "            }\n",
    "            headers = {\"User-Agent\": \"OlafJanssenBot/1.0 (https://commons.wikimedia.org/wiki/User:OlafJanssenBot; Python script)\"}\n",
    "            response = requests.get(SEARCH_URL, params=params, headers=headers).json()\n",
    "            results = response.get(\"query\", {}).get(\"search\", [])\n",
    "            all_titles.extend([\"File:\" + r[\"title\"].replace(\"File:\", \"\") for r in results])\n",
    "\n",
    "            if \"continue\" in response:\n",
    "                offset = response[\"continue\"][\"sroffset\"]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return all_titles\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search API call: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_balanced_template(wikitext, template_name):\n",
    "    \"\"\"\n",
    "    Extracts the full content of a wrapper template block (e.g., {{Photograph}}, {{Information}})\n",
    "    from the given wikitext, including nested templates and multiline fields.\n",
    "\n",
    "    This function uses brace-depth tracking to ensure the entire balanced template\n",
    "    is returned even if it contains deeply nested or multi-line sub-templates.\n",
    "\n",
    "    Args:\n",
    "        wikitext (str): The full page wikitext.\n",
    "        template_name (str): The name of the wrapper template to extract.\n",
    "\n",
    "    Returns:\n",
    "        str: The full balanced block as a string, or an empty string if not found.\n",
    "    \"\"\"\n",
    "    start = wikitext.lower().find(f'{{{{{template_name.lower()}')\n",
    "    if start == -1:\n",
    "        return ''\n",
    "\n",
    "    depth = 0\n",
    "    i = start\n",
    "    while i < len(wikitext) - 1:\n",
    "        if wikitext[i:i + 2] == '{{':\n",
    "            depth += 1\n",
    "            i += 2\n",
    "        elif wikitext[i:i + 2] == '}}':\n",
    "            depth -= 1\n",
    "            i += 2\n",
    "            if depth == 0:\n",
    "                return wikitext[start:i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return ''\n",
    "\n",
    "\n",
    "def extract_template_field(block, fieldname):\n",
    "    \"\"\"\n",
    "    Extracts a specific |field= value from a wikitext template block (multiline-safe).\n",
    "\n",
    "    Args:\n",
    "        block (str): Template content (e.g., full {{Photograph ...}} block)\n",
    "        fieldname (str): The name of the field (e.g., 'date')\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted field value or empty string if not found\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf'\\|\\s*{fieldname}\\s*=\\s*(.+?)(?=\\n\\||\\n*$)', re.IGNORECASE | re.DOTALL)\n",
    "    match = pattern.search(block)\n",
    "    return match.group(1).strip() if match else ''\n",
    "\n",
    "\n",
    "def extract_templates_and_date(wikitext):\n",
    "    \"\"\"\n",
    "    Extracts relevant license/source templates and a simplified creation date from a file's wikitext.\n",
    "\n",
    "    - Detects wrapper templates (e.g., {{Information}}, {{Photograph}}, {{Book}})\n",
    "    - Extracts |date= and |publication date= values, even if multiline\n",
    "    - Extracts embedded templates from |permission=, |date=, etc.\n",
    "    - Filters out known irrelevant or decorative templates\n",
    "    - Also finds top-level templates (not nested in wrappers)\n",
    "    - Date extraction prioritizes the most recent 4-digit year\n",
    "\n",
    "    Args:\n",
    "        wikitext (str): The raw wikitext from a Commons file\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            list[str]: Sorted list of template names (e.g. ['{{PD-old}}', '{{Anonymous-EU}}'])\n",
    "            str: Simplified creation date (e.g. '1939', 'Unknown', '20th century')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_templates = set()\n",
    "        creation_date = ''\n",
    "\n",
    "        for wrapper in WRAPPER_TEMPLATES:\n",
    "            wrapper_block = extract_balanced_template(wikitext, wrapper)\n",
    "            if wrapper_block:\n",
    "                # --- DATE ---\n",
    "                raw_date = extract_template_field(wrapper_block, 'date')\n",
    "                if raw_date:\n",
    "                    creation_date = extract_year_from_date_string(raw_date)\n",
    "                    embedded_templates = re.findall(r'\\{\\{([^\\|\\}\\n]+)', raw_date)\n",
    "                    for dt in embedded_templates:\n",
    "                        clean = f\"{{{{{dt.strip()}}}}}\"\n",
    "                        if not is_excluded_template(dt):\n",
    "                            all_templates.add(clean)\n",
    "\n",
    "                # --- PUBLICATION DATE ---\n",
    "                if not creation_date:\n",
    "                    raw_pubdate = extract_template_field(wrapper_block, 'publication date')\n",
    "                    if raw_pubdate:\n",
    "                        creation_date = extract_year_from_date_string(raw_pubdate)\n",
    "                        embedded_templates = re.findall(r'\\{\\{([^\\|\\}\\n]+)', raw_pubdate)\n",
    "                        for dt in embedded_templates:\n",
    "                            clean = f\"{{{{{dt.strip()}}}}}\"\n",
    "                            if not is_excluded_template(dt):\n",
    "                                all_templates.add(clean)\n",
    "\n",
    "                # --- PERMISSION ---\n",
    "                raw_permission = extract_template_field(wrapper_block, 'permission')\n",
    "                if raw_permission:\n",
    "                    embedded_templates = re.findall(r'\\{\\{([^\\|\\}\\n]+)', raw_permission)\n",
    "                    for t in embedded_templates:\n",
    "                        clean = f\"{{{{{t.strip()}}}}}\"\n",
    "                        if not is_excluded_template(t):\n",
    "                            all_templates.add(clean)\n",
    "\n",
    "        # --- TOP-LEVEL templates ---\n",
    "        top_level_matches = re.findall(r'^\\s*\\{\\{([^\\|\\}\\n]+)', wikitext, re.MULTILINE)\n",
    "        for t in top_level_matches:\n",
    "            clean = t.strip()\n",
    "            if not is_excluded_template(clean) and clean not in WRAPPER_TEMPLATES:\n",
    "                all_templates.add(f\"{{{{{clean}}}}}\")\n",
    "\n",
    "        return sorted(all_templates), creation_date\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting templates/date: {e}\")\n",
    "        return [], ''\n",
    "\n",
    "\n",
    "def process_templates_for_category(include_term, exclude_term):\n",
    "    \"\"\"\n",
    "    Main entry point: fetches Commons files in a target category and extracts template/date metadata.\n",
    "\n",
    "    Steps:\n",
    "    - Searches Commons for files in one category (e.g., \"Media from Delpher\") excluding another (e.g., \"Scans from the Internet Archive\").\n",
    "    - For each file, retrieves wikitext and parses relevant templates and creation dates.\n",
    "    - Outputs to console and saves results as an Excel file with URLs and template links.\n",
    "\n",
    "    Args:\n",
    "        include_term (str): Category to include (e.g., \"Media from Delpher\")\n",
    "        exclude_term (str): Category to exclude (e.g., \"Scans from the Internet Archive\")\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_titles = search_files_from_category_excluding_term(include_term, exclude_term)\n",
    "        file_titles = list(set(file_titles))  # Remove duplicates\n",
    "        records = []\n",
    "\n",
    "        for title in file_titles:\n",
    "            wikitext = get_wikitext(title)\n",
    "            file_url = \"https://commons.wikimedia.org/wiki/\" + urllib.parse.quote(title.replace(' ', '_'))\n",
    "            templates, creation_date = extract_templates_and_date(wikitext)\n",
    "\n",
    "            # Create (template, URL) pairs here\n",
    "            template_links = [\n",
    "                (tpl, f\"https://commons.wikimedia.org/wiki/Template:{tpl.strip('{}').replace(' ', '_')}\")\n",
    "                for tpl in templates\n",
    "            ]\n",
    "\n",
    "            # Console output\n",
    "            template_console = ', '.join([f\"{tpl} ({url})\" for tpl, url in template_links])\n",
    "            print(f\"{file_url} - {len(template_links)} - Date: {creation_date} - {template_console}\")\n",
    "\n",
    "            # Flatten for Excel: one row with file, date, template-URL pairs\n",
    "            row = [file_url, len(template_links), creation_date]\n",
    "            for tpl, url in template_links:\n",
    "                row.extend([tpl, url])\n",
    "            records.append(row)\n",
    "\n",
    "        if not records:\n",
    "            print(\"No valid files processed.\")\n",
    "            return\n",
    "\n",
    "        # Build Excel header\n",
    "        max_tpls = max((len(r) - 3) // 2 for r in records)  # each template = 2 columns\n",
    "        columns = ['File URL', 'NumberOfTemplates', 'DateOfCreation']\n",
    "        for i in range(max_tpls):\n",
    "            columns.extend([f'Template {i+1}', f'Template {i+1} URL'])\n",
    "\n",
    "        df = pd.DataFrame(records, columns=columns)\n",
    "\n",
    "        safe_category = include_term.replace(\" \", \"_\")  # \"Media_from_Delpher\"\n",
    "        timestamp = datetime.datetime.now().strftime(\"%d%m%Y\")\n",
    "        filename = f\"{safe_category}-Extracted_copyright_templates-{timestamp}.xlsx\"\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"Results written to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing category: {e}\")\n",
    "\n",
    "\n",
    "# Entry point for script execution.\n",
    "# When run directly (not imported), it triggers the main processing workflow\n",
    "# and catches any unhandled exceptions at the top level.\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        include_term = \"Media from Delpher\"\n",
    "        exclude_term = \"Scans from the Internet Archive\"\n",
    "        process_templates_for_category(include_term, exclude_term)\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a5f0c-690e-48e7-b0ca-08a3a1caf4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb96bf-bac8-40d3-8a67-4b6d06ad245f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
